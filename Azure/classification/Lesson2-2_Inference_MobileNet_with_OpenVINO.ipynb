{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO使ってMobileNetを推論するデモ\n",
    "\n",
    "これはOpenVINOによるディープラーニングモデルの推論デモです。OpenVINOを適用することで、インテルCPU上でモデルの推論性能が向上することを体験いただけます。\n",
    "デモで使用するモデルは、こちらの[Notebook](Demo_Inference_MobileNet_with_OpenVINO.ipynb)で作成しておりまして、具体的には犬と猫の画像を計37種類のいずれに分類するCNNモデルです。\n",
    "<table border=\"0\">\n",
    "<tr>\n",
    "<td><center>Abyssinian</center></td>\n",
    "<td><center>American pit bull</center></td>\n",
    "<td><center>Beagle</center></td>\n",
    "<td><center>Bengal</center></td>\n",
    "<td><center>Birman</center></td>\n",
    "<td><center>Boxer</center></td>\n",
    "<td><center>・・・</center></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/Abyssinian_9.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/american_pit_bull_terrier_98.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/beagle_82.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/Bengal_121.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/Birman_17.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/boxer_54.jpg\" width=\"112\"></td>\n",
    "<td><center>・・・</center><br/>(こういう感じで計37種類あります)</td>\n",
    "</tr>\n",
    "</table>\n",
    "モデルは、MobileNetで、Keras（Tensorflow バックエンド）を用いて作成（学習）され、HDF5形式でExportされています。[こちら](top_layers.mn.hdf5)がそのモデルです。\n",
    "ただし、OpenVINOのModel OptimizerはHDF5形式に対応していないため、モデルファイルをTensorflowのProtobuf形式（.pbファイル）に変換しています。それが[こちら](tf_model/top_layers.mn.pb)です。\n",
    "\n",
    "本デモでは、まずはHDF5形式のモデルをKerasを用いて複数の画像を対象に推論してみます。\n",
    "続いて、同モデルをOpeVINOのIR形式に変換し、同様の画像を対象に推論し、推論性能がどの程度変化するか確認します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まずはオリジナルのモデル（Keras + Tensorflow）の推論用関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import io\n",
    "import IPython.display\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras import models\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def convert_to_opencv(image):\n",
    "    # RGB -> BGR conversion is performed as well.\n",
    "    image = image.convert('RGB')\n",
    "    r,g,b = np.array(image).T\n",
    "    opencv_image = np.array([b,g,r]).transpose()\n",
    "    return opencv_image\n",
    "\n",
    "def crop_center(img,cropx,cropy):\n",
    "    h, w = img.shape[:2]\n",
    "    startx = w//2-(cropx//2)\n",
    "    starty = h//2-(cropy//2)\n",
    "    return img[starty:starty+cropy, startx:startx+cropx]\n",
    "\n",
    "def resize_down_to_1600_max_dim(image):\n",
    "    h, w = image.shape[:2]\n",
    "    if (h < 1600 and w < 1600):\n",
    "        return image\n",
    "\n",
    "    new_size = (1600 * w // h, 1600) if (h > w) else (1600, 1600 * h // w)\n",
    "    return cv2.resize(image, new_size, interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "def resize_to_256_square(image):\n",
    "    h, w = image.shape[:2]\n",
    "    return cv2.resize(image, (256, 256), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "def update_orientation(image):\n",
    "    exif_orientation_tag = 0x0112\n",
    "    if hasattr(image, '_getexif'):\n",
    "        exif = image._getexif()\n",
    "        if (exif != None and exif_orientation_tag in exif):\n",
    "            orientation = exif.get(exif_orientation_tag, 1)\n",
    "            # orientation is 1 based, shift to zero based and flip/transpose based on 0-based values\n",
    "            orientation -= 1\n",
    "            if orientation >= 4:\n",
    "                image = image.transpose(Image.TRANSPOSE)\n",
    "            if orientation == 2 or orientation == 3 or orientation == 6 or orientation == 7:\n",
    "                image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            if orientation == 1 or orientation == 2 or orientation == 5 or orientation == 6:\n",
    "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    return image\n",
    "\n",
    "def inference_original(total = 100):\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    labels = []\n",
    "\n",
    "    # These are set to the default names from exported models, update as needed.\n",
    "    filename = \"model.pb\"\n",
    "    labels_filename = \"labels.txt\"\n",
    "\n",
    "    # Import the TF graph\n",
    "    with tf.io.gfile.GFile(filename, 'rb') as f:\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "    # Create a list of labels.\n",
    "    with open(labels_filename, 'rt') as lf:\n",
    "        for l in lf:\n",
    "            labels.append(l.strip())\n",
    "    \n",
    "    #Read in Labels\n",
    "    arg_labels=\"mn-labels.txt\"\n",
    "    label_file = open(arg_labels, \"r\")\n",
    "    labels = label_file.read().split('\\n')\n",
    "    \n",
    "    list_df = pd.DataFrame( columns=['正解ラベル','予測ラベル','全処理時間(msec)','推論時間(msec)'] )\n",
    "\n",
    "    total_spent_time = 0\n",
    "    total_infer_spent_time = 0\n",
    "    \n",
    "    for i in range(total):\n",
    "        time1 = time.time()\n",
    "        file_list = glob.glob(\"train_data/test/*/*\")\n",
    "        img_path = random.choice(file_list)\n",
    "        img_cat = os.path.split(os.path.dirname(img_path))[1]\n",
    "        # Read and pre-process input images\n",
    "        # Load from a file\n",
    "        image = Image.open(img_path)\n",
    "        # Update orientation based on EXIF tags, if the file has orientation info.\n",
    "        image = update_orientation(image)\n",
    "        # Convert to OpenCV format\n",
    "        image = convert_to_opencv(image)\n",
    "        # If the image has either w or h greater than 1600 we resize it down respecting\n",
    "        # aspect ratio such that the largest dimension is 1600\n",
    "        image = resize_down_to_1600_max_dim(image)\n",
    "        # We next get the largest center square\n",
    "        h, w = image.shape[:2]\n",
    "        min_dim = min(w,h)\n",
    "        max_square_image = crop_center(image, min_dim, min_dim)\n",
    "        # Resize that square down to 256x256\n",
    "        augmented_image = resize_to_256_square(max_square_image)\n",
    "        # Get the input size of the model\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            input_tensor_shape = sess.graph.get_tensor_by_name('Placeholder:0').shape.as_list()\n",
    "        network_input_size = input_tensor_shape[1]\n",
    "        # Crop the center for the specified network_input_Size\n",
    "        augmented_image = crop_center(augmented_image, network_input_size, network_input_size)\n",
    "        \n",
    "        # These names are part of the model and cannot be changed.\n",
    "        output_layer = 'loss:0'\n",
    "        input_node = 'Placeholder:0'\n",
    "\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            try:\n",
    "                prob_tensor = sess.graph.get_tensor_by_name(output_layer)\n",
    "                time2 = time.time()\n",
    "                predictions, = sess.run(prob_tensor, {input_node: [augmented_image] })\n",
    "            except KeyError:\n",
    "                print (\"Couldn't find classification output layer: \" + output_layer + \".\")\n",
    "                print (\"Verify this a model exported from an Object Detection project.\")\n",
    "                exit(-1)\n",
    "        \n",
    "        infer_spent_time = time.time() - time2\n",
    "        total_infer_spent_time += infer_spent_time\n",
    "        \n",
    "        spent_time = time.time() - time1\n",
    "        total_spent_time += spent_time\n",
    "        \n",
    "        # Print the highest probability label\n",
    "        top = np.argmax(predictions)\n",
    "        pred_label = labels[top]\n",
    "        clear_output(wait=True)\n",
    "        frame = cv2.imread(img_path)\n",
    "        frame = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if frame.shape[:-1] != (h, w):\n",
    "            frame = cv2.resize(frame, (w, h))\n",
    "        cv2.putText(frame,str(i) + ':', (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(frame,str(i) + ':', (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "        cv2.putText(frame,str(img_cat), (10,80), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(frame,str(img_cat), (10,80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "        cv2.putText(frame,str(pred_label), (10,130), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(frame,str(pred_label), (10,130), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(frame).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "        tmp_se = pd.Series( [img_cat, pred_label, str(int(spent_time * 1000)), str(int(infer_spent_time * 1000)) ], index=list_df.columns )\n",
    "        list_df = list_df.append( tmp_se, ignore_index=True )\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    print()\n",
    "    print('全' + str(total) + '枚 完了！')\n",
    "    print()\n",
    "    print(\"平均処理時間: \" + str(int((total_spent_time / total)*1000.0)) + \" ms/枚\")\n",
    "    print(\"平均推論時間: \" + str(int((total_infer_spent_time / total)*1000.0)) + \" ms/枚\")\n",
    "    display(list_df)\n",
    "    \n",
    "    return int((total_spent_time / total)*1000.0), int((total_infer_spent_time / total)*1000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 続いて、Protobuf形式のモデルをOpenVINOのIR形式に変換\n",
    "\n",
    "OpenVINOに付属しているModel Optimizerを使ってモデルをIRに変換します。\n",
    "\n",
    "より詳しい説明は下記URLをご覧ください。 https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Converting_Model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model=tf_model/mobilenet.pb --input_shape=[1,224,224,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルが正常に変換されると、xmlファイルとbinファイルが出来上がります。これがOpenVINOで用いられるIR形式のモデルの実体です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ここで、IR形式のモデルを推論処理するための関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras import models\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import io\n",
    "import IPython.display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "\n",
    "def inference_openvino(total = 100, target_device=\"CPU\"):\n",
    "    model_xml = 'model.xml'\n",
    "    model_bin = 'model.bin'\n",
    "\n",
    "    # Plugin initialization for specified device and load extensions library if specified\n",
    "    # Set the desired device name as 'device' parameter. This sample support these 3 names: CPU, GPU, MYRIAD\n",
    "    ie = IEPlugin(device=target_device, plugin_dirs='')\n",
    "\n",
    "    # Read IR\n",
    "    net = IENetwork(model=model_xml, weights=model_bin)\n",
    "\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "    net.batch_size = 1\n",
    "\n",
    "    # Loading model to the plugin\n",
    "    exec_net = ie.load(network=net)\n",
    "    \n",
    "    #Read in Labels\n",
    "    arg_labels=\"mn-labels.txt\"\n",
    "    label_file = open(arg_labels, \"r\")\n",
    "    labels = label_file.read().split('\\n')\n",
    "    \n",
    "    list_df = pd.DataFrame( columns=['正解ラベル','予測ラベル','全処理時間(msec)','推論時間(msec)'] )\n",
    "\n",
    "    total_spent_time = 0\n",
    "    total_infer_spent_time = 0\n",
    "    \n",
    "    for j in range(total):\n",
    "        time1 = time.time()\n",
    "        file_list = glob.glob(\"train_data/test/*/*\")\n",
    "        img_path = random.choice(file_list)\n",
    "        img_cat = os.path.split(os.path.dirname(img_path))[1]\n",
    "        # Read and pre-process input images\n",
    "        n, c, h, w = net.inputs[input_blob].shape\n",
    "        images = np.ndarray(shape=(n, c, h, w))\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if image.shape[:-1] != (h, w):\n",
    "            image = cv2.resize(image, (w, h))\n",
    "        frame = image\n",
    "        image = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "        image = image.reshape((n, c, h, w))\n",
    "        #image = preprocess_input(image)\n",
    "        images[0] = image\n",
    "\n",
    "        # Start sync inference\n",
    "        time2 = time.time()\n",
    "        preds = exec_net.infer(inputs={input_blob: images})\n",
    "        \n",
    "        infer_spent_time = time.time() - time2\n",
    "        total_infer_spent_time += infer_spent_time\n",
    "        \n",
    "        spent_time = time.time() - time1\n",
    "        total_spent_time += spent_time\n",
    "        \n",
    "        preds = preds[out_blob]\n",
    "        top = preds[0].argsort()[-1:][::-1]\n",
    "        pred_label = labels[top[0]]\n",
    "        clear_output(wait=True)\n",
    "        cv2.putText(frame,str(j) + ':', (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(frame,str(j) + ':', (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "        cv2.putText(frame,str(img_cat), (10,80), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(frame,str(img_cat), (10,80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "        cv2.putText(frame,str(pred_label), (10,130), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(frame,str(pred_label), (10,130), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(frame).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "        #print(str(j + 1) + '枚目: 正解=' + img_cat + '、予測=' + pred_label + '、全処理時間=' + str(int(spent_time * 1000)) + 'msec、推論時間=' + str(int(infer_spent_time * 1000)) + ' msec')\n",
    "        tmp_se = pd.Series( [img_cat, pred_label, str(int(spent_time * 1000)), str(int(infer_spent_time * 1000)) ], index=list_df.columns )\n",
    "        list_df = list_df.append( tmp_se, ignore_index=True ) \n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print()\n",
    "    print('全' + str(total) + '枚 完了！')\n",
    "    print()\n",
    "    print(\"平均処理時間: \" + str(int((total_spent_time / total)*1000.0)) + \" ms/枚\")\n",
    "    print(\"平均推論時間: \" + str(int((total_infer_spent_time / total)*1000.0)) + \" ms/枚\")\n",
    "    display(list_df)\n",
    "    \n",
    "    return int((total_spent_time / total)*1000.0), int((total_infer_spent_time / total)*1000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## では、2つの関数を実行して性能を比較しましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まずは、オリジナルのモデルから実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_total_time, original_infer_time = inference_original(total=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 続いて、OpenVINOのIR形式のモデルを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_total_time, cpu_infer_time = inference_openvino(total=50, target_device=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "w = 0.4\n",
    "\n",
    "Y1 = [original_total_time - original_infer_time, cpu_total_time - cpu_infer_time]\n",
    "Y2 = [original_infer_time, cpu_infer_time]\n",
    "\n",
    "X = np.arange(len(Y1))\n",
    "\n",
    "plt.bar(X, Y1, color='gray', width=w, label='Pre/Post', align=\"center\")\n",
    "plt.bar(X, Y2, color='blue', width=w, bottom=Y1, label='Inference', align=\"center\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('MobileNet Performance Comparison')\n",
    "plt.ylabel(\"Spent time per one image (msec)\")\n",
    "\n",
    "plt.xticks(X, ['Keras (on CPU)','OpenVINO (on CPU)'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## いかがでしたでしょうか？\n",
    "## これより下は、この2つのモデルを用いた動画データに対する推論性能の比較デモです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO Video Inference on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2019 Intel Corporation.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining\n",
    "a copy of this software and associated documentation files (the\n",
    "\"Software\"), to deal in the Software without restriction, including\n",
    "without limitation the rights to use, copy, modify, merge, publish,\n",
    "distribute, sublicense, and/or sell copies of the Software, and to\n",
    "permit persons to whom the Software is furnished to do so, subject to\n",
    "the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be\n",
    "included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
    "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    "MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
    "NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n",
    "LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n",
    "OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n",
    "WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective \n",
    "Transform your frozen graph into an intermediate representation (.bin/.xml) needed to use with OpenVINO then instantiate your OpenVINO model and inference live on a video file.\n",
    "\n",
    "# Activities \n",
    "**In this section of the training you will**\n",
    "- Understand OpenVINO Arguments\n",
    "- Instantiate OpenVINO Network\n",
    "- Use OpenCV to read video and pass frames to OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import io\n",
    "import IPython.display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Network\n",
    "\n",
    "Next, we'll instantiate our network.  If you want to take a closer look at all the specific steps required to instantitate the network look at the inference.py file included in this directory.  Instead, we're going to look at the parameters passed to the constructor.  We see the .XML file passed as the base model, no CPU extensions and targeting the CPU as the device type.\n",
    "\n",
    "Then we'll call out to the constructor for the Network instantiation. We then want to load our model into that network by passing in the above parameters to load_model. Lastly, we'll read in our labels file that we're going to use to decode the results during our inference.\n",
    "\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import Network\n",
    "import sys\n",
    "\n",
    "arg_model=\"mobilenet.xml\"\n",
    "arg_cpu_extension=None\n",
    "arg_device=\"CPU\"\n",
    "\n",
    "# Initialise the class\n",
    "infer_network = Network()\n",
    "# Load the network to IE plugin to get shape of input layer\n",
    "plugin, (n_fd, c_fd, h_fd, w_fd) = infer_network.load_model(arg_model, arg_device, 1, 1, 0, arg_cpu_extension)\n",
    "\n",
    "#Read in Labels\n",
    "arg_labels=\"mn-labels.txt\"\n",
    "label_file = open(arg_labels, \"r\")\n",
    "labels = label_file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the inference!  There are 8 steps that we're doing in the code below:\n",
    "- Start a video or webcam capture\n",
    "- Read the video frame\n",
    "- Put the classification text on to the frame\n",
    "- Render the frame to the Cell output field\n",
    "- Resize/Transpose/Reshape/Preprocess frame\n",
    "- Start an inference request\n",
    "- Interpret the inference result\n",
    "- Clear frame from notebook cell\n",
    "\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture(\"persian.mp4\")\n",
    "pred_label = \"\"\n",
    "fps = 0\n",
    "ips = 0\n",
    "while True:\n",
    "    time1 = time.time()\n",
    "    ret, next_frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        next_frame = cv2.cvtColor(next_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        cv2.putText(next_frame,str(pred_label) + \" \" + str(fps) + \"fps \" + str(ips) + \"ips\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(next_frame,str(pred_label) + \" \" + str(fps) + \"fps \" + str(ips) + \"ips\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(next_frame).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "        in_frame_fd = cv2.resize(next_frame, (w_fd, h_fd))\n",
    "        in_frame_fd = in_frame_fd.transpose((2, 0, 1))\n",
    "        in_frame_fd = in_frame_fd.reshape((n_fd, c_fd, h_fd, w_fd))\n",
    "        in_frame_fd = preprocess_input(in_frame_fd)\n",
    "\n",
    "        time3 = time.time()\n",
    "        # Start asynchronous inference for specified request\n",
    "        infer_network.exec_net(0, in_frame_fd)\n",
    "        # Wait for the result\n",
    "        infer_network.wait(0)\n",
    "        # Results of the output layer of the network\n",
    "        res = infer_network.get_output(0)\n",
    "        time4 = time.time()\n",
    "\n",
    "        top = res[0].argsort()[-1:][::-1]\n",
    "        pred_label = labels[top[0]]\n",
    "\n",
    "        time2 = time.time()\n",
    "        fps = '%.1f' % (1/(time2-time1))\n",
    "        ips = '%.1f' % (1/(time4-time3))\n",
    "\n",
    "        clear_output(wait=True)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(\"Video Ended\")\n",
    "infer_network.clean()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the case that Original model is used for the same video data. You probably can see the difference of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "import time\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras import models\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "model = models.load_model('mobilenet.hdf5')\n",
    "\n",
    "cap = cv2.VideoCapture(\"persian.mp4\")\n",
    "pred_label = \"\"\n",
    "fps = 0\n",
    "ips = 0\n",
    "while True:\n",
    "    time1 = time.time()\n",
    "    ret, next_frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        next_frame = cv2.cvtColor(next_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        cv2.putText(next_frame,str(pred_label) + \" \" + str(fps) + \"fps \" + str(ips) + \"ips\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(next_frame,str(pred_label) + \" \" + str(fps) + \"fps \" + str(ips) + \"ips\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(next_frame).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "        in_frame_fd = cv2.resize(next_frame, (w_fd, h_fd))\n",
    "        #in_frame_fd = in_frame_fd.transpose((2, 0, 1))\n",
    "        in_frame_fd = in_frame_fd.reshape((n_fd, h_fd, w_fd, c_fd))\n",
    "        in_frame_fd = preprocess_input(in_frame_fd)\n",
    "\n",
    "        time3 = time.time()\n",
    "        # Start inference for specified request\n",
    "        res = model.predict(in_frame_fd)\n",
    "        time4 = time.time()\n",
    "\n",
    "        top = res[0].argsort()[-1:][::-1]\n",
    "        pred_label = labels[top[0]]\n",
    "\n",
    "        time2 = time.time()\n",
    "        fps = '%.1f' % (1/(time2-time1))\n",
    "        ips = '%.1f' % (1/(time4-time3))\n",
    "\n",
    "        clear_output(wait=True)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(\"Video Ended\")\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    " \n",
    "**In this section of the training you learned**\n",
    "- Create Intermediate Representation (.bin/.xml)\n",
    "- Understand OpenVINO Arguments\n",
    "- Instantiate OpenVINO Network\n",
    "- Use OpenCV to read video and pass frames to OpenVINO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
