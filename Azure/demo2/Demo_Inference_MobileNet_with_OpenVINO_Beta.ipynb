{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO使ってMobileNetを推論するデモ\n",
    "\n",
    "これはOpenVINOによるディープラーニングモデルの推論デモです。OpenVINOを適用することで、インテルCPU上でモデルの推論性能が向上することを体験いただけます。\n",
    "デモで使用するモデルは、こちらの[Notebook](Demo_Inference_MobileNet_with_OpenVINO.ipynb)で作成しておりまして、具体的には犬と猫の画像を計37種類のいずれに分類するCNNモデルです。\n",
    "<table border=\"0\">\n",
    "<tr>\n",
    "<td><center>Abyssinian</center></td>\n",
    "<td><center>American pit bull</center></td>\n",
    "<td><center>Beagle</center></td>\n",
    "<td><center>Bengal</center></td>\n",
    "<td><center>Birman</center></td>\n",
    "<td><center>Boxer</center></td>\n",
    "<td><center>・・・</center></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/Abyssinian_9.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/american_pit_bull_terrier_98.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/beagle_82.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/Bengal_121.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/Birman_17.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/boxer_54.jpg\" width=\"112\"></td>\n",
    "<td><center>・・・</center><br/>(こういう感じで計37種類あります)</td>\n",
    "</tr>\n",
    "</table>\n",
    "モデルは、MobileNetで、Keras（Tensorflow バックエンド）を用いて作成（学習）され、HDF5形式でExportされています。[こちら](top_layers.mn.hdf5)がそのモデルです。\n",
    "ただし、OpenVINOのModel OptimizerはHDF5形式に対応していないため、モデルファイルをTensorflowのProtobuf形式（.pbファイル）に変換しています。それが[こちら](tf_model/top_layers.mn.pb)です。\n",
    "\n",
    "本デモでは、まずはHDF5形式のモデルをKerasを用いて複数の画像を対象に推論してみます。\n",
    "続いて、同モデルをOpeVINOのIR形式に変換し、同様の画像を対象に推論し、推論性能がどの程度変化するか確認します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まずはオリジナルのモデル（Keras + Tensorflow）の推論用関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The steps implemented in the object detection sample code: \n",
    "# 1. for an image of width and height being (w, h) pixels, resize image to (w', h'), where w/h = w'/h' and w' x h' = 262144\n",
    "# 2. resize network input size to (w', h')\n",
    "# 3. pass the image to network and do inference\n",
    "# (4. if inference speed is too slow for you, try to make w' x h' smaller, which defined in object_detection.py DEFAULT_INPUT_SIZE)\n",
    "\"\"\"Sample prediction script for TensorFlow 2.x.\"\"\"\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from object_detection import ObjectDetection\n",
    "\n",
    "MODEL_FILENAME = 'model.pb'\n",
    "LABELS_FILENAME = 'labels.txt'\n",
    "\n",
    "\n",
    "class TFObjectDetection(ObjectDetection):\n",
    "    \"\"\"Object Detection class for TensorFlow\"\"\"\n",
    "\n",
    "    def __init__(self, graph_def, labels):\n",
    "        super(TFObjectDetection, self).__init__(labels)\n",
    "        self.graph = tf.compat.v1.Graph()\n",
    "        with self.graph.as_default():\n",
    "            input_data = tf.compat.v1.placeholder(tf.float32, [1, None, None, 3], name='Placeholder')\n",
    "            tf.import_graph_def(graph_def, input_map={\"Placeholder:0\": input_data}, name=\"\")\n",
    "\n",
    "    def predict(self, preprocessed_image):\n",
    "        inputs = np.array(preprocessed_image, dtype=np.float)[:, :, (2, 1, 0)]  # RGB -> BGR\n",
    "\n",
    "        with tf.compat.v1.Session(graph=self.graph) as sess:\n",
    "            output_tensor = sess.graph.get_tensor_by_name('model_outputs:0')\n",
    "            outputs = sess.run(output_tensor, {'Placeholder:0': inputs[np.newaxis, ...]})\n",
    "            return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import io\n",
    "import IPython.display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def inference_original(total = 100):\n",
    "    # Load a TensorFlow model\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile(MODEL_FILENAME, 'rb') as f:\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # Load labels\n",
    "    with open(LABELS_FILENAME, 'r') as f:\n",
    "        labels = [l.strip() for l in f.readlines()]\n",
    "\n",
    "    model = TFObjectDetection(graph_def, labels)\n",
    "    \n",
    "    #Read in Labels\n",
    "    arg_labels=\"mn-labels.txt\"\n",
    "    label_file = open(arg_labels, \"r\")\n",
    "    labels = label_file.read().split('\\n')\n",
    "    \n",
    "    list_df = pd.DataFrame( columns=['全処理時間(msec)','推論時間(msec)'] )\n",
    "\n",
    "    total_spent_time = 0\n",
    "    total_infer_spent_time = 0\n",
    "    \n",
    "    for i in range(total):\n",
    "        time1 = time.time()\n",
    "        file_list = glob.glob(\"test/*\")\n",
    "        img_path = random.choice(file_list)\n",
    "        img_cat = \"\"\n",
    "        # Read and pre-process input images\n",
    "        n, c, h, w = 1, 3, 512, 512\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if image.shape[:-1] != (h, w):\n",
    "            image = cv2.resize(image, (w, h))\n",
    "        frame = image\n",
    "        image = Image.open(img_path)\n",
    "        image = image.resize((416, 416))\n",
    "\n",
    "        time2 = time.time()\n",
    "        preds = model.predict_image(image)\n",
    "        \n",
    "        infer_spent_time = time.time() - time2\n",
    "        total_infer_spent_time += infer_spent_time\n",
    "        \n",
    "        spent_time = time.time() - time1\n",
    "        total_spent_time += spent_time\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        cv2.putText(frame,str(i) + ':', (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(frame,str(i) + ':', (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "        processedimage = model.preprocess(image)\n",
    "        for pred in preds:\n",
    "            left = pred['boundingBox']['left']\n",
    "            top = pred['boundingBox']['top']\n",
    "            pred_label = \"sneaker\" + \" - \" + str(int(pred['probability']*100.0)) + '%'\n",
    "            cv2.putText(frame,str(pred_label), (int(processedimage.width * left),int(processedimage.height * top)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,2550), 4)\n",
    "            cv2.putText(frame,str(pred_label), (int(processedimage.width * left),int(processedimage.height * top)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 2)\n",
    "        \n",
    "        frame = cv2.resize(frame, (processedimage.width, processedimage.height))\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(frame).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "        tmp_se = pd.Series( [str(int(spent_time * 1000)), str(int(infer_spent_time * 1000)) ], index=list_df.columns )\n",
    "        list_df = list_df.append( tmp_se, ignore_index=True )\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    print()\n",
    "    print('全' + str(total) + '枚 完了！')\n",
    "    print()\n",
    "    print(\"平均処理時間: \" + str(int((total_spent_time / total)*1000.0)) + \" ms/枚\")\n",
    "    print(\"平均推論時間: \" + str(int((total_infer_spent_time / total)*1000.0)) + \" ms/枚\")\n",
    "    display(list_df)\n",
    "    \n",
    "    return int((total_spent_time / total)*1000.0), int((total_infer_spent_time / total)*1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f5a35f714863>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moriginal_total_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_infer_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference_original\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-f0bb2590c65c>\u001b[0m in \u001b[0;36minference_original\u001b[1;34m(total)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mtime2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0minfer_spent_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\usr\\dev\\github\\OpenVINO_Sample\\demo3\\object_detection.py\u001b[0m in \u001b[0;36mpredict_image\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[0mprediction_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-dcf8001d8627>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, preprocessed_image)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_outputs:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Placeholder:0'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\oss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\oss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\oss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\oss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\oss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1317\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\oss\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1352\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "original_total_time, original_infer_time = inference_original(total=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-9.56548899e-02  1.58902735e-01 -5.96256971e-01 ...  9.74949375e-02\n",
      "   -7.42444515e+00  9.78912354e-01]\n",
      "  [ 1.58350751e-01 -2.42983252e-01 -3.12658697e-01 ...  9.18721706e-02\n",
      "   -7.70286179e+00  4.23524857e-01]\n",
      "  [ 3.48358959e-01 -1.69251412e-01 -4.17966574e-01 ...  7.67179132e-02\n",
      "   -8.18552685e+00  1.98691893e+00]\n",
      "  ...\n",
      "  [ 2.41579488e-01 -9.07927230e-02 -4.72534895e-01 ...  3.13563123e-02\n",
      "   -7.53951073e+00 -1.31786823e-01]\n",
      "  [ 1.87016696e-01 -9.99460146e-02 -5.50278962e-01 ...  1.85796097e-02\n",
      "   -7.56952858e+00  2.76284695e-01]\n",
      "  [ 4.36658800e-01  2.28204340e-01 -8.52187395e-01 ...  6.88726082e-02\n",
      "   -7.89396286e+00  2.02050066e+00]]\n",
      "\n",
      " [[-3.50446820e-01  1.35309681e-01 -1.04036164e+00 ...  4.41562533e-02\n",
      "   -7.12092209e+00 -1.98491955e+00]\n",
      "  [-7.05366880e-02  1.54551461e-01 -6.58375263e-01 ...  1.11472160e-02\n",
      "   -8.22978592e+00 -2.34278536e+00]\n",
      "  [-2.20469803e-01  1.60379678e-01 -5.33266187e-01 ...  5.26882112e-02\n",
      "   -9.06345177e+00  7.38987446e-01]\n",
      "  ...\n",
      "  [ 8.04148093e-02  1.18979946e-01 -1.01147532e+00 ... -2.67143101e-02\n",
      "   -7.64046574e+00 -2.26110077e+00]\n",
      "  [ 7.87610561e-02  3.77645671e-01 -9.62588191e-01 ... -2.94909626e-03\n",
      "   -7.49288368e+00 -2.06878424e+00]\n",
      "  [ 4.09171462e-01  4.79814440e-01 -1.48792064e+00 ...  8.02626386e-02\n",
      "   -7.33786774e+00 -4.60761547e-01]]\n",
      "\n",
      " [[-5.29054344e-01 -1.20655976e-01 -1.21398246e+00 ...  5.97342327e-02\n",
      "   -7.82384968e+00 -1.12310362e+00]\n",
      "  [ 1.86793864e-01 -2.52764136e-01 -5.35167634e-01 ... -2.35329792e-02\n",
      "   -9.58725929e+00 -1.01023960e+00]\n",
      "  [ 2.65214533e-01 -6.33337617e-01 -8.39440942e-01 ... -6.28428534e-03\n",
      "   -1.12553177e+01  4.20908976e+00]\n",
      "  ...\n",
      "  [ 3.17281902e-01 -4.97584194e-01 -1.23063958e+00 ... -1.16421327e-01\n",
      "   -8.90345478e+00  7.75091648e-01]\n",
      "  [ 1.64016381e-01 -3.43099833e-01 -8.92248690e-01 ... -5.58893792e-02\n",
      "   -7.85734272e+00 -1.09893608e+00]\n",
      "  [ 5.23314714e-01 -1.09084524e-01 -1.40241337e+00 ...  5.48114702e-02\n",
      "   -7.28607559e+00 -7.19022274e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-3.76525313e-01  2.56267369e-01 -1.56441879e+00 ... -1.33607596e-01\n",
      "   -7.94075203e+00  4.49942827e+00]\n",
      "  [ 8.19367766e-02  6.79978967e-01 -1.08371329e+00 ... -2.56698191e-01\n",
      "   -8.18236923e+00  3.36263132e+00]\n",
      "  [ 3.00157040e-01  1.36669111e+00 -1.21839094e+00 ... -2.83403337e-01\n",
      "   -9.06446362e+00  3.23424768e+00]\n",
      "  ...\n",
      "  [ 5.39799869e-01 -1.82142526e-01 -2.85011560e-01 ... -3.37269783e-01\n",
      "   -1.12983017e+01  8.49865913e+00]\n",
      "  [-6.35454178e-01 -2.53827184e-01 -5.66893339e-01 ... -1.65546402e-01\n",
      "   -9.58098888e+00  4.58431673e+00]\n",
      "  [ 5.77797353e-01  1.56528622e-01 -1.64874792e+00 ... -7.28298426e-02\n",
      "   -7.28270531e+00  4.70918989e+00]]\n",
      "\n",
      " [[ 1.30015120e-01  6.82378560e-03 -9.97381687e-01 ... -1.51863948e-01\n",
      "   -8.65767479e+00  5.05801535e+00]\n",
      "  [ 1.19442761e+00  1.13274284e-01 -2.22109973e-01 ... -2.14720726e-01\n",
      "   -7.85899925e+00  2.02077055e+00]\n",
      "  [-1.27622038e-01 -5.24437368e-01  5.48757792e-01 ... -2.05282316e-01\n",
      "   -7.94854450e+00  5.97648144e-01]\n",
      "  ...\n",
      "  [ 3.93263310e-01 -7.70129859e-01 -1.46576986e-01 ... -2.42726937e-01\n",
      "   -1.12986755e+01  8.92462540e+00]\n",
      "  [-7.94817358e-02 -7.06969857e-01 -2.28000656e-01 ... -1.11433096e-01\n",
      "   -1.00395117e+01  5.76717138e+00]\n",
      "  [ 8.02709162e-01  2.97537446e-03 -1.70857370e+00 ... -1.15104634e-02\n",
      "   -7.56001186e+00  4.55299616e+00]]\n",
      "\n",
      " [[-4.42874938e-01  1.98414773e-01 -4.57177341e-01 ...  2.78369896e-02\n",
      "   -8.07223797e+00  7.01379156e+00]\n",
      "  [ 5.83564162e-01 -1.29328281e-01 -5.14464676e-02 ... -2.60999538e-02\n",
      "   -7.79877949e+00  5.45360041e+00]\n",
      "  [-3.33794542e-02 -6.15380704e-01  4.60332870e-01 ... -7.14926571e-02\n",
      "   -8.97570229e+00  6.24194384e+00]\n",
      "  ...\n",
      "  [ 1.16244424e-02  1.51689351e+00  6.12665653e-01 ... -2.25936919e-02\n",
      "   -9.00687885e+00  8.07961464e+00]\n",
      "  [-2.20481545e-01  1.14137268e+00  9.46240783e-01 ...  1.24415576e-01\n",
      "   -9.12119865e+00  6.13734293e+00]\n",
      "  [ 1.07498455e+00  2.09137440e-01 -9.45261717e-01 ...  1.51824281e-01\n",
      "   -7.96692944e+00  4.31428289e+00]]]\n",
      "0.86546755\n",
      "{'left': 0.10929907, 'top': 0.82300992, 'width': 0.21647673, 'height': 0.10830812}\n"
     ]
    }
   ],
   "source": [
    "# Load a TensorFlow model\n",
    "graph_def = tf.compat.v1.GraphDef()\n",
    "with tf.io.gfile.GFile(MODEL_FILENAME, 'rb') as f:\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "# Load labels\n",
    "with open(LABELS_FILENAME, 'r') as f:\n",
    "    labels = [l.strip() for l in f.readlines()]\n",
    "\n",
    "od_model = TFObjectDetection(graph_def, labels)\n",
    "\n",
    "image = Image.open(\"test/sample.jpg\")\n",
    "predictions = od_model.predict_image(image)\n",
    "print(predictions[0]['probability'])\n",
    "print(predictions[0]['boundingBox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protobuf形式のモデルをOpenVINOのIR形式に変換\n",
    "\n",
    "OpenVINOに付属しているModel Optimizerを使って。\n",
    "大前提として、OSの環境変数に下記がセットされている必要があります。\n",
    "\n",
    "PYTHONPATH=c:\\Program Files (x86)\\IntelSWTools\\openvino\\python\\python3.6;c:\\Program Files (x86)\\IntelSWTools\\openvino\\python\\python3;\n",
    "\n",
    "より詳しい説明は下記URLをご覧ください。 https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Converting_Model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \tC:\\usr\\dev\\github\\OpenVINO_Sample\\demo3\\model.pb\n",
      "\t- Path for generated IR: \tC:\\usr\\dev\\github\\OpenVINO_Sample\\demo3\\.\n",
      "\t- IR output name: \tmodel_fp32\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t[1,416,416,3]\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \tNone\n",
      "\t- Operations to offload: \tNone\n",
      "\t- Patterns to offload: \tNone\n",
      "\t- Use the config file: \tNone\n",
      "Model Optimizer version: \t2019.2.0-436-gf5827d4\n",
      "\n",
      "[ SUCCESS ] Generated IR model.\n",
      "[ SUCCESS ] XML file: C:\\usr\\dev\\github\\OpenVINO_Sample\\demo3\\.\\model_fp32.xml\n",
      "[ SUCCESS ] BIN file: C:\\usr\\dev\\github\\OpenVINO_Sample\\demo3\\.\\model_fp32.bin\n",
      "[ SUCCESS ] Total execution time: 9.88 seconds. \n"
     ]
    }
   ],
   "source": [
    "!python \"c:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\model_optimizer\\mo.py\" --input_model=model.pb --input_shape=[1,416,416,3] --model_name=model_fp32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この後で同モデルをNeural Compute Stick2上で実行するため、FP16形式にも変換しておきます。（NCS2はFP16のみ対応のため）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"c:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\model_optimizer\\mo.py\" --input_model=model.pb --input_shape=[1,416,416,3] --data_type=FP16 --model_name=model_fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルが正常に変換されると、xmlファイルとbinファイルが出来上がります。これがOpenVINOで用いられるIR形式のモデルの実体です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ここで、IR形式のモデルを推論処理するための関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras import models\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import io\n",
    "import IPython.display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "\n",
    "def inference_openvino(total = 100, target_device=\"CPU\", precision=\"FP32\"):\n",
    "    if precision==\"FP16\":\n",
    "        model_xml = 'model_fp16.xml'\n",
    "        model_bin = 'model_fp16.bin'\n",
    "    else:\n",
    "        model_xml = 'model_fp32.xml'\n",
    "        model_bin = 'model_fp32.bin'\n",
    "\n",
    "    # Plugin initialization for specified device and load extensions library if specified\n",
    "    # Set the desired device name as 'device' parameter. This sample support these 3 names: CPU, GPU, MYRIAD\n",
    "    ie = IEPlugin(device=target_device, plugin_dirs='')\n",
    "\n",
    "    # Read IR\n",
    "    net = IENetwork(model=model_xml, weights=model_bin)\n",
    "\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "    net.batch_size = 1\n",
    "\n",
    "    # Loading model to the plugin\n",
    "    exec_net = ie.load(network=net)\n",
    "    \n",
    "    #Read in Labels\n",
    "    arg_labels=\"labels.txt\"\n",
    "    label_file = open(arg_labels, \"r\")\n",
    "    labels = label_file.read().split('\\n')\n",
    "    \n",
    "    list_df = pd.DataFrame( columns=['正解ラベル','予測ラベル','全処理時間(msec)','推論時間(msec)'] )\n",
    "\n",
    "    total_spent_time = 0\n",
    "    total_infer_spent_time = 0\n",
    "    \n",
    "    for j in range(total):\n",
    "        time1 = time.time()\n",
    "        file_list = glob.glob(\"test/*\")\n",
    "        img_path = random.choice(file_list)\n",
    "        img_cat = os.path.split(os.path.dirname(img_path))[1]\n",
    "        # Read and pre-process input images\n",
    "        n, c, h, w = net.inputs[input_blob].shape\n",
    "        images = np.ndarray(shape=(n, c, h, w))\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if image.shape[:-1] != (h, w):\n",
    "            image = cv2.resize(image, (w, h))\n",
    "        frame = image\n",
    "        image = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "        image = image.reshape((n, c, h, w))\n",
    "        images[0] = image\n",
    "\n",
    "        # Start sync inference\n",
    "        time2 = time.time()\n",
    "        preds = exec_net.infer(inputs={input_blob: images})\n",
    "        \n",
    "        infer_spent_time = time.time() - time2\n",
    "        total_infer_spent_time += infer_spent_time\n",
    "        \n",
    "        spent_time = time.time() - time1\n",
    "        total_spent_time += spent_time\n",
    "        \n",
    "        preds = preds[out_blob]\n",
    "        preds = od_model.postprocess(preds[0].transpose(1, 2, 0))\n",
    "        clear_output(wait=True)\n",
    "        cv2.putText(frame,str(j) + ':', (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(frame,str(j) + ':', (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "        \n",
    "        for pred in preds:\n",
    "            left = pred['boundingBox']['left']\n",
    "            top = pred['boundingBox']['top']\n",
    "            pred_label = \"sneaker\" + \" - \" + str(int(pred['probability']*100.0)) + '%'\n",
    "            cv2.putText(frame,str(pred_label), (int(w * left),int(h * top)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,2550), 4)\n",
    "            cv2.putText(frame,str(pred_label), (int(w * left),int(h * top)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 2)\n",
    "            \n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(frame).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "        #print(str(j + 1) + '枚目: 正解=' + img_cat + '、予測=' + pred_label + '、全処理時間=' + str(int(spent_time * 1000)) + 'msec、推論時間=' + str(int(infer_spent_time * 1000)) + ' msec')\n",
    "        tmp_se = pd.Series( [img_cat, pred_label, str(int(spent_time * 1000)), str(int(infer_spent_time * 1000)) ], index=list_df.columns )\n",
    "        list_df = list_df.append( tmp_se, ignore_index=True ) \n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print()\n",
    "    print('全' + str(total) + '枚 完了！')\n",
    "    print()\n",
    "    print(\"平均処理時間: \" + str(int((total_spent_time / total)*1000.0)) + \" ms/枚\")\n",
    "    print(\"平均推論時間: \" + str(int((total_infer_spent_time / total)*1000.0)) + \" ms/枚\")\n",
    "    display(list_df)\n",
    "    \n",
    "    return int((total_spent_time / total)*1000.0), int((total_infer_spent_time / total)*1000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## では、2つの関数を実行して性能を比較しましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まずは、オリジナルのモデルから実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_total_time, original_infer_time = inference_original(total=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 続いて、OpenVINOのIR形式のモデルを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "全500枚 完了！\n",
      "\n",
      "平均処理時間: 37 ms/枚\n",
      "平均推論時間: 26 ms/枚\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>正解ラベル</th>\n",
       "      <th>予測ラベル</th>\n",
       "      <th>全処理時間(msec)</th>\n",
       "      <th>推論時間(msec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>47</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>44</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>36</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>34</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>36</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>44</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>35</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>36</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>38</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>36</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>34</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>46</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>49</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>34</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>45</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>47</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>44</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>47</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>46</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>44</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>43</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>74</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>44</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>38</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>45</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>40</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>38</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>45</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>38</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>test</td>\n",
       "      <td>sneaker - 14%</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    正解ラベル          予測ラベル 全処理時間(msec) 推論時間(msec)\n",
       "0    test  sneaker - 14%          38         29\n",
       "1    test  sneaker - 14%          47         37\n",
       "2    test  sneaker - 14%          44         32\n",
       "3    test  sneaker - 14%          35         26\n",
       "4    test  sneaker - 14%          35         26\n",
       "5    test  sneaker - 14%          36         24\n",
       "6    test  sneaker - 14%          32         24\n",
       "7    test  sneaker - 14%          35         27\n",
       "8    test  sneaker - 14%          34         26\n",
       "9    test  sneaker - 14%          34         25\n",
       "10   test  sneaker - 14%          36         27\n",
       "11   test  sneaker - 14%          34         27\n",
       "12   test  sneaker - 14%          38         29\n",
       "13   test  sneaker - 14%          36         27\n",
       "14   test  sneaker - 14%          44         23\n",
       "15   test  sneaker - 14%          34         25\n",
       "16   test  sneaker - 14%          35         25\n",
       "17   test  sneaker - 14%          34         25\n",
       "18   test  sneaker - 14%          36         28\n",
       "19   test  sneaker - 14%          36         27\n",
       "20   test  sneaker - 14%          38         30\n",
       "21   test  sneaker - 14%          34         24\n",
       "22   test  sneaker - 14%          38         30\n",
       "23   test  sneaker - 14%          38         27\n",
       "24   test  sneaker - 14%          36         26\n",
       "25   test  sneaker - 14%          35         26\n",
       "26   test  sneaker - 14%          38         29\n",
       "27   test  sneaker - 14%          36         28\n",
       "28   test  sneaker - 14%          42         30\n",
       "29   test  sneaker - 14%          33         23\n",
       "..    ...            ...         ...        ...\n",
       "470  test  sneaker - 14%          36         25\n",
       "471  test  sneaker - 14%          34         24\n",
       "472  test  sneaker - 14%          34         22\n",
       "473  test  sneaker - 14%          34         25\n",
       "474  test  sneaker - 14%          46         36\n",
       "475  test  sneaker - 14%          49         35\n",
       "476  test  sneaker - 14%          40         27\n",
       "477  test  sneaker - 14%          35         24\n",
       "478  test  sneaker - 14%          34         24\n",
       "479  test  sneaker - 14%          34         23\n",
       "480  test  sneaker - 14%          45         36\n",
       "481  test  sneaker - 14%          47         35\n",
       "482  test  sneaker - 14%          47         34\n",
       "483  test  sneaker - 14%          44         35\n",
       "484  test  sneaker - 14%          47         36\n",
       "485  test  sneaker - 14%          46         36\n",
       "486  test  sneaker - 14%          44         34\n",
       "487  test  sneaker - 14%          43         32\n",
       "488  test  sneaker - 14%          74         58\n",
       "489  test  sneaker - 14%          44         32\n",
       "490  test  sneaker - 14%          38         28\n",
       "491  test  sneaker - 14%          45         28\n",
       "492  test  sneaker - 14%          40         28\n",
       "493  test  sneaker - 14%          38         27\n",
       "494  test  sneaker - 14%          35         24\n",
       "495  test  sneaker - 14%          45         33\n",
       "496  test  sneaker - 14%          38         27\n",
       "497  test  sneaker - 14%          36         24\n",
       "498  test  sneaker - 14%          40         27\n",
       "499  test  sneaker - 14%          40         27\n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cpu_total_time, cpu_infer_time = inference_openvino(total=500, target_device=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "w = 0.4\n",
    "\n",
    "Y1 = [original_total_time - original_infer_time, cpu_total_time - cpu_infer_time]\n",
    "Y2 = [original_infer_time, cpu_infer_time]\n",
    "\n",
    "X = np.arange(len(Y1))\n",
    "\n",
    "plt.bar(X, Y1, color='gray', width=w, label='Pre/Post', align=\"center\")\n",
    "plt.bar(X, Y2, color='blue', width=w, bottom=Y1, label='Inference', align=\"center\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('MobileNet Performance Comparison')\n",
    "plt.ylabel(\"Spent time per one image (msec)\")\n",
    "\n",
    "plt.xticks(X, ['Keras (on CPU)','OpenVINO (on CPU)'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 同じIR形式のモデルを内臓GPUに推論処理をオフロードして実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_total_time, gpu_infer_time = inference_openvino(total=50, target_device=\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 同じIR形式のモデルをNCS2(Neural Compute Stick (Movidius))に推論処理をオフロードして実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncs_total_time, ncs_infer_time = inference_openvino(total=50, target_device=\"MYRIAD\", precision=\"FP16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果をグラフ化して比較してみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "w = 0.4\n",
    "\n",
    "Y1 = [original_total_time - original_infer_time, cpu_total_time - cpu_infer_time, gpu_total_time - gpu_infer_time, ncs_total_time - ncs_infer_time]\n",
    "Y2 = [original_infer_time, cpu_infer_time, gpu_infer_time, ncs_infer_time]\n",
    "\n",
    "X = np.arange(len(Y1))\n",
    "\n",
    "plt.bar(X, Y1, color='gray', width=w, label='Pre/Post', align=\"center\")\n",
    "plt.bar(X, Y2, color='blue', width=w, bottom=Y1, label='Inference', align=\"center\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('MobileNet Performance Comparison')\n",
    "plt.ylabel(\"Spent time per one image (msec)\")\n",
    "\n",
    "plt.xticks(X, ['Keras(CPU)','OpenVINO(CPU)','OpenVINO(iGPU)','OpenVINO(NCS2)'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## いかがでしたでしょうか？\n",
    "## これより下は、この2つのモデルを用いた動画データに対する推論性能の比較デモです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO Video Inference on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2019 Intel Corporation.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining\n",
    "a copy of this software and associated documentation files (the\n",
    "\"Software\"), to deal in the Software without restriction, including\n",
    "without limitation the rights to use, copy, modify, merge, publish,\n",
    "distribute, sublicense, and/or sell copies of the Software, and to\n",
    "permit persons to whom the Software is furnished to do so, subject to\n",
    "the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be\n",
    "included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
    "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    "MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
    "NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n",
    "LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n",
    "OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n",
    "WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective \n",
    "Transform your frozen graph into an intermediate representation (.bin/.xml) needed to use with OpenVINO then instantiate your OpenVINO model and inference live on a video file.\n",
    "\n",
    "# Activities \n",
    "**In this section of the training you will**\n",
    "- Understand OpenVINO Arguments\n",
    "- Instantiate OpenVINO Network\n",
    "- Use OpenCV to read video and pass frames to OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import io\n",
    "import IPython.display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Network\n",
    "\n",
    "Next, we'll instantiate our network.  If you want to take a closer look at all the specific steps required to instantitate the network look at the inference.py file included in this directory.  Instead, we're going to look at the parameters passed to the constructor.  We see the .XML file passed as the base model, no CPU extensions and targeting the CPU as the device type.\n",
    "\n",
    "Then we'll call out to the constructor for the Network instantiation. We then want to load our model into that network by passing in the above parameters to load_model. Lastly, we'll read in our labels file that we're going to use to decode the results during our inference.\n",
    "\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import Network\n",
    "import sys\n",
    "\n",
    "arg_model=\"top_layers.mn.xml\"\n",
    "arg_cpu_extension=None\n",
    "arg_device=\"CPU\"\n",
    "\n",
    "# Initialise the class\n",
    "infer_network = Network()\n",
    "# Load the network to IE plugin to get shape of input layer\n",
    "plugin, (n_fd, c_fd, h_fd, w_fd) = infer_network.load_model(arg_model, arg_device, 1, 1, 0, arg_cpu_extension)\n",
    "\n",
    "#Read in Labels\n",
    "arg_labels=\"mn-labels.txt\"\n",
    "label_file = open(arg_labels, \"r\")\n",
    "labels = label_file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the inference!  There are 8 steps that we're doing in the code below:\n",
    "- Start a video or webcam capture\n",
    "- Read the video frame\n",
    "- Put the classification text on to the frame\n",
    "- Render the frame to the Cell output field\n",
    "- Resize/Transpose/Reshape/Preprocess frame\n",
    "- Start an inference request\n",
    "- Interpret the inference result\n",
    "- Clear frame from notebook cell\n",
    "\n",
    "Click the cell below and then click **Run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet import preprocess_input\n",
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture(\"persian.mp4\")\n",
    "pred_label = \"\"\n",
    "fps = 0\n",
    "ips = 0\n",
    "while True:\n",
    "    time1 = time.time()\n",
    "    ret, next_frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        next_frame = cv2.cvtColor(next_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        cv2.putText(next_frame,str(pred_label) + \" \" + str(fps) + \"fps \" + str(ips) + \"ips\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(next_frame,str(pred_label) + \" \" + str(fps) + \"fps \" + str(ips) + \"ips\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(next_frame).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "        in_frame_fd = cv2.resize(next_frame, (w_fd, h_fd))\n",
    "        in_frame_fd = in_frame_fd.transpose((2, 0, 1))\n",
    "        in_frame_fd = in_frame_fd.reshape((n_fd, c_fd, h_fd, w_fd))\n",
    "        in_frame_fd = preprocess_input(in_frame_fd)\n",
    "\n",
    "        time3 = time.time()\n",
    "        # Start asynchronous inference for specified request\n",
    "        infer_network.exec_net(0, in_frame_fd)\n",
    "        # Wait for the result\n",
    "        infer_network.wait(0)\n",
    "        # Results of the output layer of the network\n",
    "        res = infer_network.get_output(0)\n",
    "        time4 = time.time()\n",
    "\n",
    "        top = res[0].argsort()[-1:][::-1]\n",
    "        pred_label = labels[top[0]]\n",
    "\n",
    "        time2 = time.time()\n",
    "        fps = '%.1f' % (1/(time2-time1))\n",
    "        ips = '%.1f' % (1/(time4-time3))\n",
    "\n",
    "        clear_output(wait=True)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(\"Video Ended\")\n",
    "infer_network.clean()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the case that Original model is used for the same video data. You probably can see the difference of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet import preprocess_input\n",
    "import time\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras import models\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "model = models.load_model('top_layers.mn.hdf5')\n",
    "\n",
    "cap = cv2.VideoCapture(\"persian.mp4\")\n",
    "pred_label = \"\"\n",
    "fps = 0\n",
    "ips = 0\n",
    "while True:\n",
    "    time1 = time.time()\n",
    "    ret, next_frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        next_frame = cv2.cvtColor(next_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        cv2.putText(next_frame,str(pred_label) + \" \" + str(fps) + \"fps \" + str(ips) + \"ips\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(next_frame,str(pred_label) + \" \" + str(fps) + \"fps \" + str(ips) + \"ips\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(next_frame).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "        in_frame_fd = cv2.resize(next_frame, (w_fd, h_fd))\n",
    "        #in_frame_fd = in_frame_fd.transpose((2, 0, 1))\n",
    "        in_frame_fd = in_frame_fd.reshape((n_fd, h_fd, w_fd, c_fd))\n",
    "        in_frame_fd = preprocess_input(in_frame_fd)\n",
    "\n",
    "        time3 = time.time()\n",
    "        # Start inference for specified request\n",
    "        res = model.predict(in_frame_fd)\n",
    "        time4 = time.time()\n",
    "\n",
    "        top = res[0].argsort()[-1:][::-1]\n",
    "        pred_label = labels[top[0]]\n",
    "\n",
    "        time2 = time.time()\n",
    "        fps = '%.1f' % (1/(time2-time1))\n",
    "        ips = '%.1f' % (1/(time4-time3))\n",
    "\n",
    "        clear_output(wait=True)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(\"Video Ended\")\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    " \n",
    "**In this section of the training you learned**\n",
    "- Create Intermediate Representation (.bin/.xml)\n",
    "- Understand OpenVINO Arguments\n",
    "- Instantiate OpenVINO Network\n",
    "- Use OpenCV to read video and pass frames to OpenVINO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
